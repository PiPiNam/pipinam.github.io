---
layout:     post                    # 使用的布局（不需要改）
title:      阿南带你理解决策树        # 标题 
subtitle:   阿南的决策树             # 副标题
date:       2020-02-04              # 时间
author:     Nam                     # 作者
header-img: img/post-bg-decision_tree-highway.jpg    # 这篇文章标题背景图片
mathjax: true                       # 使用mathjax
catalog: true                       # 是否归档
tags:                               # 标签
    - 彻底理解机器学习
---

# 理解决策树

# 前言

大家好，我是阿南，我先自我介绍一下吧。我本身并不是科班出身，本科就读的是EE，现在在香港城市大学攻读数据科学硕士学位。

在学习机器学习方法的过程之中，我也遇到了很多困难。比方说数据结构的概念不熟悉，或是数学公式的不理解。如果不彻底弄懂其算法原理及其推到，那么顶多算是一个只能念出百度百科里基本概念的半吊子，在找工作面试时只能被呛。因此，在这里，我尽量用通俗易懂的大白话帮助大家理解常见的机器学习方法，以数学公式与实际例子相结合的方式。

第一篇机器学习的博客我希望从数据结构里面的树来写起，希望我的这个自我梳理能够帮助到大家理解决策树这个方法。

# 正文

## 决策树的直观理解

首先，从字面意思上去理解它。 “决策”就是做决定。“树”就是一个有着主干和众多分支的结构。所以”决策树“可以理解为不断地在分叉路口做出决定，最终勇敢地走到终点，找到属于自己的那个Label。

作为海南人，我举一个生活中挑选椰子的例子吧！面对众多的椰子，我们的目标是挑选出一个又大又甜的椰子。

![又甜又大的椰子](https://s2.ax1x.com/2020/02/20/3m1Xge.jpg)

首先我会观察椰子的大小，如果椰子足够大那么我就进行下一步判断，否则直接将目光放到下一个椰子上。

接着在大椰子的基础上，我会继续观察这个椰子的尾部。如果这个椰子的尾部是平的，即整体比较圆，那么此时可以判断出这个椰子的内核比较大，反之若尾部比较尖则内核小，但是此时还不能确定里面的椰子水的总量。

我们把椰子拎起来选择一个重量大的，然后摇一摇，再选择一个没有明显的晃动水声的椰子，这样的椰子内核水分保存相对完好。

最后，我们再用指甲掐一下椰子的表皮，若出水则说明该椰子比较嫩，其中的椰子水不会很甜，反之若皮较干则说明是一个大龄椰子，水分含糖量高。

![阿南的选椰子决策树](https://s2.ax1x.com/2020/02/20/3m1O3D.jpg)

此时，一个相对”完美“的椰子就选择完毕了。在这个过程中可以发现，我们是在许多个判断中摸索前进最终到达终点得到这个椰子的好坏结论的。观察这个图片我们不难发现，它就像一颗倒过来从上往下逐渐变得枝繁叶茂的树，这也就是决策树中”树“这个名字的来源~

这个时候问题来了，大家可能会问，那我怎么决定哪个特征用来判断呢？以及这些特征出现的先后顺序又怎么决定呢？

这个时候我们就要引入一个概念：**熵**

>***熵*** (entropy)，是用来表示随机变量不确定性的度量。设 $X$ 是一个取有限值的离散随机变量，其概率分布为：
$$ P(X=x_i) = p_i, i=1,2,...,n $$

则随机变量 $X$ 的熵定义为：

$$ H(X)=-\sum_{i=1}^{n}p_i\log{p_i} $$

我们可以从公式中看出，如果变量的不确定性越大，那么其熵也就越大！

解释一下就是，如果一件事发生的概率是0或是1，那么我们就可以很确定的说这件事会不会发生。反之，如果这件事发生的概率是0.5，那么即可能发生也可能不发生，此时就存在很大的不确定性。

>接着我们再引入***条件熵*** (conditional entropy)， 其定义为：在已知随机变量B的情况下随机变量A的不确定性

$$ H(A|B)=\sum_{i=1}^{n}{p_iH(A|B)}=-\sum_{i=1}^{n}\frac{|C_k|}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D_i|}\log_2\frac{|D_{ik}|}{|D_i|} $$

举个栗子:

$$ H(椰子好坏|椰子重量) $$

其描述的就是在已知椰子重量的条件下椰子质量的不确定性。从上面的描述我们即使知道了椰子的重量，仍然还存在许多因素影响椰子的质量，所以上式的数值会比较大。
所以如果我们能够找到一个特征能够使得其条件熵比较小的话，则说明该特征的引入减少了该数据集的“混乱程度”，换句话说就是我们可以更容易的找到正确结果。

>信息增益 $g(D, A)$,**表示得知特征 $A$ 之后而使得数据集 $D$ 中类别的不确定性的减少程度**。

公式定义为集合D的经验熵 $H(D)$ 与特征A给定条件下D的经验条件熵 $H(D\mid A)$之差，即

$$ g(D, A)=H(D) - H(D|A) $$

结合上式以及我们之前对于熵和条件熵的定义我们可以看出，信息增益越大说明特征 $A$ 的加入能够更加准确地找到对应的类别。

此时我们就掌握了根据信息增益来选择特征去划分数据集以构建决策树的方法了！

>1. 判断数据集中Label的种类是否 > 1, 若数据集中所有实例同属于一种类别，则树$T$为单节点树，直接输出该类；
>2. 判断数据集中的特征种类是否 > 0， 若特征为空集，则将实例数最大的类别作为该节点的类标记；
>3. 即遍历计算数据集中各个特征的信息增益，排序，选择出信息增益最大的特征 $A_g$；
>4. 若数据集$A_g$的信息增益都小于阈值 $\epsilon$，我们就将该数据集中数量最多的Label作为该节点的Label；
>5. 否则，按照特征$A_g$的取值$a_i$, 按照$A_g=a_i$将数据集划分为若干个子数据集，构建子节点；
>6. 随后将子数据集中的特征 $A$ 剔除，对各个子数据集递归执行步骤 1->5。

至此，生成决策树的 **ID3** 算法就介绍完毕啦。

在这里其实还有一个可以改进的地方。通过观察条件熵的概念我们可以发现，若某一个特征$A_g$的取值个数较多，那么该特征的信息增益就会越大，就会导致存在偏向于选择取值较多的特征的问题。为了矫正这一个问题，我们可以选择使用 ***信息增益比 (information gain ratio)***来作为特征选择的准则。

>***信息增益比*** ： 信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征$A$的值的熵 $H_A{(D)}$ 之比。

$$ g_{R}(D, A)=\frac{g(D,A)}{H_{A}(D)} $$

蟹蟹大家的耐心观看！欢迎大家提出意见和建议~

（评论系统正在建设中！请耐心等待~）

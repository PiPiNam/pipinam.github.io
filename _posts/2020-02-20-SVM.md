---
layout:     post                    # 使用的布局（不需要改）
title:      阿南带你理解支持向量机    # 标题 
subtitle:   向量也支持阿南~        # 副标题
date:       2020-02-20              # 时间
author:     Nam                     # 作者
header-img: img/post-bg-SVM-highway.jpg    # 这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               # 标签
    - 彻底理解机器学习
---

# 前言

关于支持向量机的博客我很久之前就想下笔写了，由于拖延症的关系一直放到现在 -.-! 今天下笔写这篇博客，也是为了已经打响的春招做准备！现在受疫情以及经济大环境不景气的情况下，要找到一份称心的工作着实要下一番功夫！而支持向量机又是算法、机器学习和人工智能岗位必考的模型。

好了，让我们开始进入面试中最为热门的支持向量机部分吧！ Let us go!

# 正文

## 支持向量机 (Support Vector Machine, SVM)

支持向量机，一个听起来特学术、特唬人的名字，实际上我们之前早已接触过它的兄弟模型 ***感知机 (Perceptron)***（详情请见 [阿南带你理解感知机](https://pipinam.github.io/2020/02/09/Perceptron/)）。相比于感知机要求 **数据集必须线性可分** 的约束来说，支持向量机就不存在这个约束，这个我们将在后面的核函数中提到。以下给出支持向量机的的定义：

>**支持向量机：** 是一种二分类模型，其目的是在输入的特征空间中找到一个最大 ***间隔 (margin)*** 的决策超平面，其中 ***间隔最大*** 使得支持向量机有别于感知机。 此外，支持向量机是通过直接学习最终的判别函数 $y=f(x)$，故属于 **判别模型**。

### 线性可分支持向量机

给定一个线性可分的数据集：

$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\} 
$$

其中，$x_i\in \chi=R^n, y_i\in \gamma =\{+1,-1\}$。其中$x_i$是数据集中第$i$个**特征向量**、**样本**或称之为**实例**，$y_i$为$x_i$的类标记。

对于线性可分的数据集，感知机的策略是利用最小化误分类的原则求出决策超平面，此时符合条件的决策超平面是无穷多个。而支持向量机则由于采用了 **最大化间隔** 的策略，因此只存在一个最优的决策超平面。

那么，什么是间隔呢？如何找到最大间隔的决策超平面呢？

一般来说，样本与分离超平面的距离对应于这个样本属于特定类别的确信度。换句话说就是，离间隔越远就越”安全“，被误判的可能性就越小。于是有了 ***函数间隔 (functional margin)*** 的概念：

***函数间隔 (functional margin)：*** 对给定的数据集 $T$  和超平面 $(w,b)$， 定义超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的函数间隔为：

$$
\hat \gamma_i =y_i(w\cdot x_i +b) \tag{1}
$$

定义超平面 $(w,b)$ 关于数据集 $T$ 的函数间隔为超平面 $(w,b)$ 中关于数据集 $T$ 内所有样本点 $(x_i,y_i)$ 的函数间隔中的最小值：

$$
\hat \gamma = \min_{i=1,2,...,N} \gamma_i \tag{2}
$$

然鹅，单纯的函数间隔并不足以用来确定决策超平面，因为如果成比例的改变  $w$ 和 $b$ 例如将其统一增加至 $2w$ 和 $2b$ 所有实例的函数间隔都会变为原来的两倍。注意，这里是数据集中所有实例的函数间隔都翻倍了，也就是所有实例与决策超平面之间的相对距离没有改变。

我们的目的是要找到一个间隔最大的决策超平面，如果可以通过改变 $w$ 和 $b$ 来改变函数距离的话，那么就不能用函数间隔去找到最优的决策超平面了。因为支持向量机是要找到一个间隔最大的超平面，但是我们总能通过改变 $w$ 和 $b$ 来改变函数距离甚至可以使得相对最小的函数间隔 $\hat \gamma$ 都变得无穷大，导致我们不能将最大化函数间隔作为我们的优化目标。

为了解决这个问题，我们引入了几何间隔的概念：

$$
\gamma = \frac{\hat \gamma}{||w||} = \frac{w}{||w||}\cdot x_i + \frac{b}{||w||} \tag{3}
$$

在这里我们将函数间隔除以 $\mid \mid w\mid \mid$ 转变为几何间隔，这样无论如何成比例的改变  $w$ 和 $b$ 都不会影响最终的间隔。因此我们取 $\hat \gamma=1$，便得到如下最优化问题：

$$
\begin{aligned}
& \min_{w,b} \frac{1}{2}||w||^2 \\
& s.t. \ \ \  y_i(w\cdot x_i +b)-1\ge 0
\end{aligned} \tag{4}
$$

我来简单地解释一下这其中的转变。刚开始我们的优化目标是最大化函数间隔 $\hat \gamma$，但是后来发现函数间隔会随着 $w, b$ 一起成比例的改变，无法作为优化目标。

于是我们转变为最大化几何间隔 $\gamma = \frac{\hat \gamma}{\mid \mid w\mid \mid}$，并且为了方便起见，我们 nomarlize 函数间隔为 $1$ 即 $\hat \gamma = 1$。此时我们的目标就是 $\max \frac{1}{\mid \mid w\mid \mid}$


同时，$\max \frac{1}{\mid \mid w\mid \mid} \equiv \min \mid \mid w\mid \mid \equiv \min \frac{1}{2} \mid \mid w\mid \mid^2$。最后加上常数系数 $\frac{1}{2}$ 是为了后续计算的方便，并不影响优化的结果。

我们发现上述问题是 ***凸二次规划*** 问题，这为后面的求解提供了理论支持。

凸优化问题是指约束最优化问题：

$$
\begin{aligned}
& \min_w f(w) \\
& s.t. \ \ \ g_i(w)\le 0 \\
& \ \ \ \ \ \ \ \ h_i(w)=0
\end{aligned} \tag{5}
$$

期中，目标函数 $f(w)$ 和不等式约束 $g_i(w)\le 0$ 都是 $R^n$ 上的连续可微的凸函数，约束函数 $h_i(w)$ 是 $R^n$ 上满足 $h(w) = a\cdot w + b$ 的仿射函数。

当目标函数 $f(w)$ 是二次函数且约束函数 $g_i(w)$ 也是仿射函数时，上述凸优化问题就转化为凸二次规划问题。

因此我们的目标很简单，只需根据上述优化问题计算出解 $w^{\*},b^{\*}$ 就得到了决策超平面 $w^{\*} \cdot x + b^{\*}=0$ 以及分类决策函数 $f(x)=sign(w^{\*}\cdot x + b^{\*})$。

## 软间隔最大化的线性支持向量机

然而，现实并不总是那么的美好，我们面对的数据集往往是线性不可分的。也就是说，我们不可能找到一个完美的分离超平面能够把两类样本完全真确地分离开。因此最大化间隔也就无从谈起了。

![线性不可分](https://s2.ax1x.com/2020/02/20/3mlNQJ.png)

不过没关系，阿南总能想出办法~ 我们稍微降低自己滴要求，不要幻想我们可以把所有样本都能正确地分离开。我们要允许模型 ***“犯少量的错”***，这样我们就能在犯少量错的前提下依旧找到间隔最大化的决策超平面！

因为线性不可分就意味着某些样本点不能够满足函数间隔大于 $1$ 的约束条件（注意，我们在线性可分支持向量机的优化部分 $(4)$ 中已将函数间隔归一化为 $1$）。因此，在线性可分支持向量机的基础上，我们引入 ***松弛变量 $\xi$ (slack variable)***。约束条件就变为：

$$
y_i(w\cdot x_i +b) \ge 1-\xi_i \tag{6}
$$

同时，为了避免模型犯太多错误，我们需要让模型为每一次错误付出代价 $\xi_i$。此时目标函数由原来的 $\frac{1}{2}\mid \mid w\mid \mid^2$ 变为：

$$
\frac{1}{2}||w||^2 + C\sum_{i=1}^{N}\xi_i \tag{7}
$$

上式 $(7)$ 中的 $C>0$ 为惩罚参数，用来模型对错误的容忍度。因为我们的目标就是最小化目标函数，因此越大的 $C$ 对应的就是越小的 $\sum_{i=1}^{N}\xi_i$，也就是越不能容忍犯错，同时我们还要最小化 $\frac{1}{2}\mid \mid w\mid \mid^2$ 也就是最大化间隔。

线性不可分的线性支持向量机的学习策略就转变成如下凸二次规划问题：

$$
\begin{aligned}
& \min_{w,b,\xi}\frac{1}{2}||w||^2 + C\sum_{i=1}^{N}\xi_i \\
& s.t. \ \ \ y_i(w\cdot x_i +b)\ge 1-\xi_i \\
& \ \ \ \ \ \ \ \ \xi_i\ge 0
\end{aligned} \tag{8}
$$

对凸二次规划问题 $(8)$ 构建拉格朗日函数：

$$
\begin{aligned}
& L(w,b,\xi,\alpha,\mu) = \frac{1}{2}||w||^2 + C\sum_{i=1}^{N}\xi_i - \sum_{i=1}^{N}\alpha_i(y_i(w\cdot x_i +b)-1+\xi_i)- \sum_{i=1}^{N}\mu_i\xi_i \\
& s.t. \ \ \alpha_i(y_i(w\cdot x_i + b)-1 +\xi_i) = 0 \\
& \ \ \ \ \ \ \ \ \mu_i \xi_i = 0 \\
& \ \ \ \ \ \ \ \ y_i(w\cdot x_i + b) -1 + \xi_i \ge 0 \\
& \ \ \ \ \ \ \ \ \alpha \ge 0 \\
& \ \ \ \ \ \ \ \ \mu_i \ge 0 \\
& \ \ \ \ \ \ \ \ \xi_i \ge 0
\end{aligned} \tag{9}
$$

能够使用强对偶需要满足两个条件：
1. **严格条件**
2. **KKT条件**

严格条件是指原始问题是凸函数，约束条件是仿射函数。

在满足 $KKT$ 条件的前提下，根据拉格朗日对偶性，可以将原始问题 $(9)$ 转变为对偶的广义拉格朗日函数的极大极小问题：

$$
\max_{\alpha} \min_{w,b,\xi} L(w,b,\xi,\alpha,\mu) \tag{10}
$$

***对偶问题 (dual problem)*** $(9)$ 与 ***原始问题 (primary problem)*** $(10)$ 是等价的，即它们拥有相同的解 $w^{\*}, b^{\*}$。 为了得到最优解 $w^{\*}, b^{\*}$，我们先求 $L(w,b,\xi,\alpha,\mu)$ 对 $w,b,\xi$ 的极小：

$$
\begin{aligned}
& \nabla_w L(w,b,\xi,\alpha,\mu) = w - \sum_{i=1}^{N} \alpha_i y_i x_i = 0 \\
& \nabla_b L(w,b,\xi,\alpha,\mu) = -\sum_{i=1}^{N}\alpha_i y_i = 0 \\
& \nabla_{\xi_i} L(w,b,\xi,\alpha,\mu) = C-\alpha_i-\mu_i=0
\end{aligned}
$$

得到：

$$
w=\sum_{i=1}^{N}\alpha_iy_ix_i \tag{11}
$$

$$
\sum_{i=1}^{N} \alpha_iy_i = 0 \tag{12}
$$

$$
C-\alpha_i -\mu_i = 0 \tag{13}
$$

将结果 $(11)~(13)$ 带入至 $(9)$ 中，得到：

$$
\max_{w,b,\xi} L(w,b,\xi,\alpha,\mu) = -\frac{1}{2} \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum_{i=1}^{N}\alpha_i \tag{14}
$$

再对 $(14)$ 求关于 $\alpha$ 的极大，即得到对偶问题：

$$
\begin{aligned}
& \min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^{N}\alpha_i \\
& s.t. \ \ \ \sum_{i=1}^{N}\alpha_i y_j=0 \\
& \ \ \ \ \ \ \ \ C-\alpha_i -\mu_i = 0 \\ 
& \ \ \ \ \ \ \ \ \alpha_i \ge 0 \\
& \ \ \ \ \ \ \ \ \mu_i\ge 0
\end{aligned} \tag{15}
$$

此处我们仔细观察式 $(15)$ 的约束条件，可以将最后三个约束条件统一写成：

$$
0\le \alpha_i \le C
$$

同时我们把 $(15)$ 中的求极大通过乘以 $-1$ 转变为求极小，即：

$$
\min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^{N}\alpha_i \tag{16}
$$

OK，此时我们就可以给出线性支持向量机的学习算法了~

**输入：** 训练数据集 $T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$，其中，$x_i\in \chi=R^n, y_i\in \gamma =\{+1,-1\}$。其中$x_i$是数据集中第$i$个**特征向量**、**样本**或称之为**实例**，$y_i$为$x_i$的类标记。

**输出：** 决策超平面和分类决策函数。

1. 设定惩罚参数 $C$ 的初始值，根据 $(15),(16)$ 构造如下凸二次规划问题:

$$
\begin{aligned}
& \min_{\alpha} \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum_{i=1}^{N}\alpha_i \\
& s.t. \ \ \ \sum_{i=1}^{N}\alpha_iy_i=0 \\
& \ \ \ \ \ \ \ \ \  0\le \alpha_i \le C
\end{aligned}
$$
2. 计算 
   $$w^* = \sum_{i=1}^{N}\alpha_i^*y_ix_i$$ 
   选择 $\alpha_i$ 的一个分量 $\alpha_j^*$ 适合条件 $0\le \alpha_j^*\le C$，计算：
    $$
    b^*=y_j-\sum_{i=1}^{N}y_i\alpha_i^*(x_i\cdot x_j)
    $$

3. 得出决策超平面：
   $$
   w^* \cdot x + b^* =0
   $$
   以及我们一直以来所追求的分类决策函数：
   $$
   f(x) = sign(w^* \cdot x + b^*)
   $$

在第二步，关于 $w$ 的解来自 $(11)$，对 $b$ 的解来自 $(9)$ 中第一个等式约束以及 $y^2 = 1$ 的结论。

其实，讲到现在都还没有提到 ***支持向量机*** 中的 ***支持向量*** 是什么意思。这是一个非常重要的概念，在面试中如果提到支持向量机一定会问到这个概念。

> ***支持向量：*** 对决策超平面的求解有贡献的向量，称之为 **支持向量 (support vector)**。 
> 
> 对应等式约束 $\alpha_i(y_i(w\cdot x_i + b)-1 +\xi_i) = 0$ 中，所有 $\alpha_i > 0$ 的实例均是支持向量。

下图中红色的点即为支持向量。

![支持向量图解](https://s2.ax1x.com/2020/02/20/3mlYz4.png)

如下是对支持向量的归纳：
1. 间隔上的实例
2. 间隔错误一侧的实例
3. 决策超平面上的实例
4. 决策超平面错误一侧的实例

接下来关于 ***非线性支持向量机*** 以及 ***核函数 (kernel function)*** 的部分我准备在随后有更深的理解之后再进行更新~

谢谢大家！

:)
